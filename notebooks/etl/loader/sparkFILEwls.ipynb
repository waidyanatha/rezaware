{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e182df41",
   "metadata": {},
   "source": [
    "# Spark Files Read/Write ClassTester\n",
    "\n",
    "Test the DataIO controller that serves as wrapper to read and write data from and to a particular file system.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "* Spark Hadoop 3.0 or latest\n",
    "* [AWS jars files](https://stackoverflow.com/questions/44411493/java-lang-noclassdeffounderror-org-apache-hadoop-fs-storagestatistics) must be in $SPARK_HOME/jars  folder. The jar versions must match (e.g. 3.3.4 or 1.12.376 as below)\n",
    "   * hadoop-aws-3.3.4.jar\n",
    "   * hadoop-common-3.3.4.jar\n",
    "   * aws-java-sdk-s3-1.12.376.jar\n",
    "   * aws-java-sdk-dynamodb-1.12.376.jar\n",
    "   * aws-java-sdk-core-1.12.376.jar\n",
    "   * jets3t-0.7.1.jar\n",
    "   * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94df7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0a12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKFILEWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKFILEWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "sparkFILEwls Class initialization complete\n",
      "\n",
      "read and write files from and to a particular source class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('rezaware/')[0])\n",
    "from rezaware.modules.etl.loader import sparkFILEwls as spark\n",
    "from rezaware.modules.etl.loader.sparkFILEwls import credentials as cred\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    spark= importlib.reload(spark)\n",
    "\n",
    "__desc__ = \"read and write files from and to a particular source\"\n",
    "clsSpark = spark.FileWorkLoads(desc=__desc__)\n",
    "if clsSpark._session:\n",
    "    clsSpark._session.stop\n",
    "print(\"\\n%s class initialization and load complete!\" % __desc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e31e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = aws-s3-bucket\n",
      "jar = /opt/spark_hadoop_3/jars/aws-java-sdk-s3-1.12.376.jar\n"
     ]
    }
   ],
   "source": [
    "# clsSpark.storeMode = \"local-fs\"\n",
    "clsSpark.storeMode = \"aws-s3-bucket\"\n",
    "# clsSpark.storeMode = \"google-storage\"\n",
    "print(\"mode =\",clsSpark.storeMode)\n",
    "\n",
    "''' set the driver '''\n",
    "if clsSpark.storeMode.lower() == \"local-fs\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"postgresql-42.6.0.jar\")\n",
    "elif clsSpark.storeMode.lower() == \"aws-s3-bucket\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"aws-java-sdk-s3-1.12.376.jar\")\n",
    "elif clsSpark.storeMode.lower() == \"google-storage\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"gcs-connector-hadoop3-2.2.10.jar\")\n",
    "else:\n",
    "    pass\n",
    "print(\"jar =\",clsSpark.jarDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170c8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_root = rezgcorp-data-science\n"
     ]
    }
   ],
   "source": [
    "### local-fs\n",
    "# clsSpark.storeRoot = proj_dir.split('rezaware/')[0]\n",
    "### AWS S3\n",
    "clsSpark.storeRoot = \"rezgcorp-data-science\"\n",
    "### google cloud store\n",
    "# clsSpark.storeRoot = \"tip-daily-marketcap\"   #\"rezaware-wrangler-source-code\"\n",
    "print(\"data_root =\",clsSpark.storeRoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f92ced",
   "metadata": {},
   "source": [
    "## Read from storage and return as dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229f7421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/06 21:05:10 WARN Utils: Your hostname, FarmRaider2 resolves to a loopback address: 127.0.1.1; using 192.168.124.14 instead (on interface enp3s0)\n",
      "23/07/06 21:05:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/07/06 21:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkFILEwls Class initialization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/06 21:05:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got back 61747 rows with return dtype pandas\n",
      "           Search Date Check-in Date Check-out Date  \\\n",
      "0  02-23-2023 12:00:09    2023-02-23     2023-02-24   \n",
      "1  02-23-2023 12:00:09    2023-02-23     2023-02-24   \n",
      "2  02-23-2023 12:00:09    2023-02-23     2023-02-24   \n",
      "3  02-23-2023 12:00:09    2023-02-23     2023-02-24   \n",
      "4  02-23-2023 12:00:09    2023-02-23     2023-02-24   \n",
      "\n",
      "                                               Hotel Star Rating(out of 5)  \\\n",
      "0                              Caribe Royale Orlando                     4   \n",
      "1                       Garnet Inn & Suites, Orlando                  None   \n",
      "2  Red Roof Inn PLUS+ Orlando - Convention Center...                     3   \n",
      "3                      Quality Inn & Suites Downtown                     2   \n",
      "4  Travelodge by Wyndham Orlando at Heart of Inte...                     2   \n",
      "\n",
      "                                           Room Type    Rate  Review Score  \\\n",
      "0               Newly Re-Imagined Caribe Queen Suite  US$302           8.8   \n",
      "1                                         Queen Room   US$77           6.1   \n",
      "2                    Business King Room - Smoke Free   US$89           5.4   \n",
      "3                               King Room - Smoking    US$80           5.3   \n",
      "4  Double Room with Two Double Beds and Pool View...   US$70           5.1   \n",
      "\n",
      "                       Location                          Room Details  \\\n",
      "0     Lake Buena Vista, Orlando  3 beds (1 sofa bed, 2 large doubles)   \n",
      "1                       Orlando   2 beds (1 sofa bed, 1 large double)   \n",
      "2  International Drive, Orlando              1 extra-large double bed   \n",
      "3                       Orlando              1 extra-large double bed   \n",
      "4  International Drive, Orlando                         2 double beds   \n",
      "\n",
      "  Breakfast Cancellations                                 Availability  \n",
      "0      None          None                                         None  \n",
      "1       yes          None                                         None  \n",
      "2      None          None                                         None  \n",
      "3       yes          None  Only 4 rooms left at this price on our site  \n",
      "4       yes          None                                         None  \n"
     ]
    }
   ],
   "source": [
    "__as_type__ = \"pandas\"  # specify data type to return from read file\n",
    "''' set the folder path relative the storeRoot '''\n",
    "_folder_path = \"Booking Data/2023-02-23_12/\"\n",
    "''' set the single file name '''\n",
    "_file_name = None  #file name to read from or write data to; e.g. myfile.csv\n",
    "''' set the file type to read all files of specified type in folder '''\n",
    "_file_type = \"csv\"   # specify file type to write data to or read from\n",
    "''' options key value pairs must match the pyspark standard'''\n",
    "options = {\n",
    "    \"inferSchema\":True,\n",
    "    \"header\":True,\n",
    "    \"delimiter\":\",\",\n",
    "    \"pathGlobFilter\":'*.csv',\n",
    "    \"recursiveFileLookup\":True,\n",
    "}\n",
    "\n",
    "_data = clsSpark.read_files_to_dtype(\n",
    "    as_type=__as_type__,\n",
    "    folder_path=_folder_path,\n",
    "    file_name=_file_name,\n",
    "    file_type=_file_type,\n",
    "    **options,\n",
    ")\n",
    "if __as_type__ == 'pandas':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(_data.shape[0],__as_type__))\n",
    "    print(_data.head(5))\n",
    "elif __as_type__ == 'spark':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(_data.count(),__as_type__))\n",
    "    print(_data.show(5,truncate=False))\n",
    "elif __as_type__ == 'dict':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(len(_data),__as_type__))\n",
    "    print(_data[:3])\n",
    "else:\n",
    "    print('Unrecognised __as_type__' % __as_type__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8d890",
   "metadata": {},
   "source": [
    "## Filter and save data in storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a74efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 60 filtered rows to file savetodummy.csv in rezaware/data/etl/loader/sampledata/\n",
      "saved to <class 'str'> \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "_filter_data = _data.loc[_data['asset_symbol'].isin(['1inch','zks'])]\n",
    "__file_name__=\"savetodummy.csv\"\n",
    "print(\"writing %d filtered rows to file %s in %s\" \n",
    "      % (_filter_data.shape[0],__file_name__,__local_folder_path__))\n",
    "write_data=clsSpark.write_data(\n",
    "    file_name=__file_name__,\n",
    "    folder_path=__local_folder_path__,\n",
    "    data=_filter_data\n",
    ")\n",
    "\n",
    "print(\"saved to %s \" % write_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e86256f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def write_read(bucket_name, blob_name):\n",
    "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your new GCS object\n",
    "    # blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Mode can be specified as wb/rb for bytes mode.\n",
    "    # See: https://docs.python.org/3/library/io.html\n",
    "#     with blob.open(\"w\") as f:\n",
    "#         f.write(\"Hello world\")\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        print(f.read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
