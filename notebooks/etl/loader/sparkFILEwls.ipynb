{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e182df41",
   "metadata": {},
   "source": [
    "# Spark Files Read/Write ClassTester\n",
    "\n",
    "Test the DataIO controller that serves as wrapper to read and write data from and to a particular file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94df7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0a12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKFILEWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "sparkFILEwls Class initialization complete\n",
      "\n",
      "read and write files from and to a particular source class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('rezaware/')[0])\n",
    "from rezaware.modules.etl.loader import sparkFILEwls as spark\n",
    "from rezaware.modules.etl.loader.sparkFILEwls import credentials as cred\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    spark= importlib.reload(spark)\n",
    "\n",
    "__desc__ = \"read and write files from and to a particular source\"\n",
    "clsSpark = spark.FileWorkLoads(desc=__desc__)\n",
    "if clsSpark._session:\n",
    "    clsSpark._session.stop\n",
    "print(\"\\n%s class initialization and load complete!\" % __desc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17aa55c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark_hadoop_3/\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e31e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = local-fs\n",
      "jar = /opt/spark_hadoop_3/jars/postgresql-42.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "clsSpark.storeMode = \"local-fs\"\n",
    "# clsSpark.storeMode = \"aws-s3-bucket\"\n",
    "# clsSpark.storeMode = \"google-storage\"\n",
    "print(\"mode =\",clsSpark.storeMode)\n",
    "\n",
    "''' set the driver '''\n",
    "if clsSpark.storeMode.lower() == \"local-fs\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"postgresql-42.6.0.jar\")\n",
    "elif clsSpark.storeMode.lower() == \"aws-s3-bucket\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"aws-java-sdk-s3-1.12.376.jar\")\n",
    "elif clsSpark.storeMode.lower() == \"google-storage\":\n",
    "    clsSpark.jarDir = os.path.join(os.environ['SPARK_HOME'],\"jars\",\n",
    "                                   \"gcs-connector-hadoop3-2.2.10.jar\")\n",
    "else:\n",
    "    pass\n",
    "print(\"jar =\",clsSpark.jarDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170c8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_root = /home/nuwan/workspace/TIP/\n"
     ]
    }
   ],
   "source": [
    "clsSpark.storeRoot = proj_dir.split('rezaware/')[0]\n",
    "# clsSpark.storeRoot = \"rezaware-wrangler-source-code\"\n",
    "# clsSpark.storeRoot = \"tip-daily-marketcap\"   #\"rezaware-wrangler-source-code\"\n",
    "print(\"data_root =\",clsSpark.storeRoot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f92ced",
   "metadata": {},
   "source": [
    "## Read from storage and return as dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229f7421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 15:51:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "23/06/14 15:51:33 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
      "23/06/14 15:51:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/06/14 15:51:33 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#77, None)) > 0)\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 200.2 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.124.14:39997 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:33 INFO SparkContext: Created broadcast 12 from load at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5412341 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/06/14 15:51:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Got job 7 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Final stage: ResultStage 9 (load at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[30] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.1 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.124.14:39997 (size: 6.0 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[30] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:33 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (192.168.124.14, executor driver, partition 0, PROCESS_LOCAL, 7965 bytes) \n",
      "23/06/14 15:51:33 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
      "23/06/14 15:51:33 INFO FileScanRDD: Reading File path: file:///home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/sparkfilewls.csv, range: 0-1218037, partition values: [empty row]\n",
      "23/06/14 15:51:33 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 1703 bytes result sent to driver\n",
      "23/06/14 15:51:33 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 50 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:33 INFO DAGScheduler: ResultStage 9 (load at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Job 7 finished: load at NativeMethodAccessorImpl.java:0, took 0.066138 s\n",
      "23/06/14 15:51:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/06/14 15:51:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 200.2 KiB, free 433.3 MiB)\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.124.14:39997 (size: 34.5 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:33 INFO SparkContext: Created broadcast 14 from load at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5412341 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/06/14 15:51:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Got job 8 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Final stage: ResultStage 10 (load at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 26.4 KiB, free 433.2 MiB)\n",
      "23/06/14 15:51:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 433.2 MiB)\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 192.168.124.14:39997 (size: 12.4 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:33 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.124.14:39997 in memory (size: 34.5 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:33 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (192.168.124.14, executor driver, partition 0, PROCESS_LOCAL, 7965 bytes) \n",
      "23/06/14 15:51:33 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)\n",
      "23/06/14 15:51:33 INFO FileScanRDD: Reading File path: file:///home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/sparkfilewls.csv, range: 0-1218037, partition values: [empty row]\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.124.14:39997 in memory (size: 6.0 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:33 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.124.14:39997 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:33 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1714 bytes result sent to driver\n",
      "23/06/14 15:51:33 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 279 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:33 INFO DAGScheduler: ResultStage 10 (load at NativeMethodAccessorImpl.java:0) finished in 0.298 s\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/14 15:51:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "23/06/14 15:51:33 INFO DAGScheduler: Job 8 finished: load at NativeMethodAccessorImpl.java:0, took 0.303760 s\n",
      "23/06/14 15:51:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/06/14 15:51:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 200.1 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 192.168.124.14:39997 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 16 from count at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5412341 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Registering RDD 40 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Got map stage job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[40] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.9 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 192.168.124.14:39997 (size: 8.6 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[40] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (192.168.124.14, executor driver, partition 0, PROCESS_LOCAL, 7954 bytes) \n",
      "23/06/14 15:51:34 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)\n",
      "23/06/14 15:51:34 INFO FileScanRDD: Reading File path: file:///home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/sparkfilewls.csv, range: 0-1218037, partition values: [empty row]\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.124.14:39997 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 1973 bytes result sent to driver\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 39 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:34 INFO DAGScheduler: ShuffleMapStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.055 s\n",
      "23/06/14 15:51:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/06/14 15:51:34 INFO DAGScheduler: running: Set()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: waiting: Set()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: failed: Set()\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 192.168.124.14:39997 in memory (size: 12.4 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Got job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Final stage: ResultStage 13 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 12.1 KiB, free 433.7 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.7 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 192.168.124.14:39997 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[43] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (192.168.124.14, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/06/14 15:51:34 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)\n",
      "23/06/14 15:51:34 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/06/14 15:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/06/14 15:51:34 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 3995 bytes result sent to driver\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 11 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:34 INFO DAGScheduler: ResultStage 13 (count at NativeMethodAccessorImpl.java:0) finished in 0.025 s\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 10 finished: count at NativeMethodAccessorImpl.java:0, took 0.030308 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 15:51:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/06/14 15:51:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 200.1 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 192.168.124.14:39997 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 19 from count at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5412341 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Registering RDD 47 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Got map stage job 11 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 16.9 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 192.168.124.14:39997 (size: 8.6 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (192.168.124.14, executor driver, partition 0, PROCESS_LOCAL, 7954 bytes) \n",
      "23/06/14 15:51:34 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)\n",
      "23/06/14 15:51:34 INFO FileScanRDD: Reading File path: file:///home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/sparkfilewls.csv, range: 0-1218037, partition values: [empty row]\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 192.168.124.14:39997 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 1973 bytes result sent to driver\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 39 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:34 INFO DAGScheduler: ShuffleMapStage 14 (count at NativeMethodAccessorImpl.java:0) finished in 0.050 s\n",
      "23/06/14 15:51:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/06/14 15:51:34 INFO DAGScheduler: running: Set()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: waiting: Set()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: failed: Set()\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 192.168.124.14:39997 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 192.168.124.14:39997 in memory (size: 8.6 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Got job 12 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Final stage: ResultStage 16 (count at NativeMethodAccessorImpl.java:0)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[50] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.1 KiB, free 433.7 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 433.7 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 192.168.124.14:39997 (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[50] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 12) (192.168.124.14, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/06/14 15:51:34 INFO Executor: Running task 0.0 in stage 16.0 (TID 12)\n",
      "23/06/14 15:51:34 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/06/14 15:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "23/06/14 15:51:34 INFO Executor: Finished task 0.0 in stage 16.0 (TID 12). 3995 bytes result sent to driver\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 12) in 11 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:34 INFO DAGScheduler: ResultStage 16 (count at NativeMethodAccessorImpl.java:0) finished in 0.022 s\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 12 finished: count at NativeMethodAccessorImpl.java:0, took 0.026923 s\n",
      "23/06/14 15:51:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/06/14 15:51:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.1 KiB, free 433.5 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 192.168.124.14:39997 (size: 34.5 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 22 from toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108\n",
      "23/06/14 15:51:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5412341 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/06/14 15:51:34 INFO SparkContext: Starting job: toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Got job 13 (toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108) with 1 output partitions\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Final stage: ResultStage 17 (toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108)\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Missing parents: List()\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[53] at toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108), which has no missing parents\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 11.8 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.4 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 192.168.124.14:39997 (size: 6.4 KiB, free: 434.2 MiB)\n",
      "23/06/14 15:51:34 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[53] at toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108) (first 15 tasks are for partitions Vector(0))\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (192.168.124.14, executor driver, partition 0, PROCESS_LOCAL, 7965 bytes) \n",
      "23/06/14 15:51:34 INFO Executor: Running task 0.0 in stage 17.0 (TID 13)\n",
      "23/06/14 15:51:34 INFO FileScanRDD: Reading File path: file:///home/nuwan/workspace/TIP/rezaware/data/etl/loader/sampledata/sparkfilewls.csv, range: 0-1218037, partition values: [empty row]\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 192.168.124.14:39997 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 192.168.124.14:39997 in memory (size: 8.6 KiB, free: 434.3 MiB)\n",
      "23/06/14 15:51:34 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 192.168.124.14:39997 in memory (size: 34.5 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 15:51:34 INFO Executor: Finished task 0.0 in stage 17.0 (TID 13). 180482 bytes result sent to driver\n",
      "23/06/14 15:51:34 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 199 ms on 192.168.124.14 (executor driver) (1/1)\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "23/06/14 15:51:34 INFO DAGScheduler: ResultStage 17 (toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108) finished in 0.210 s\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/06/14 15:51:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "23/06/14 15:51:34 INFO DAGScheduler: Job 13 finished: toPandas at /home/nuwan/workspace/TIP/rezaware/modules/etl/loader/sparkFILEwls.py:1108, took 0.212777 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got back 6278 rows with return dtype pandas\n",
      "   mcap_past_pk                      uuid asset_symbol  mcap_date  \\\n",
      "0         17392  64535d5d4a6e0a5e3a6399cd        1inch 2023-03-01   \n",
      "1         17393  64535d5d4a6e0a5e3a6399d3        1inch 2023-03-02   \n",
      "2         17394  64535d5d4a6e0a5e3a6399f2        1inch 2023-03-03   \n",
      "3         17395  64535d5d4a6e0a5e3a639a0c        1inch 2023-03-04   \n",
      "4         17396  64535d5d4a6e0a5e3a639a28        1inch 2023-03-05   \n",
      "\n",
      "     mcap_value asset_name currency              created_dt  created_by  \\\n",
      "0  4.969543e+08      1inch      usd 2023-05-05 07:25:50.866  farmraider   \n",
      "1  4.915683e+08      1inch      usd 2023-05-05 07:25:50.866  farmraider   \n",
      "2  4.708599e+08      1inch      usd 2023-05-05 07:25:50.866  farmraider   \n",
      "3  4.432800e+08      1inch      usd 2023-05-05 07:25:50.866  farmraider   \n",
      "4  4.400357e+08      1inch      usd 2023-05-05 07:25:50.866  farmraider   \n",
      "\n",
      "                                        created_proc  \n",
      "0  wrangler_assets_etl_CryptoMarket function <nos...  \n",
      "1  wrangler_assets_etl_CryptoMarket function <nos...  \n",
      "2  wrangler_assets_etl_CryptoMarket function <nos...  \n",
      "3  wrangler_assets_etl_CryptoMarket function <nos...  \n",
      "4  wrangler_assets_etl_CryptoMarket function <nos...  \n"
     ]
    }
   ],
   "source": [
    "__as_type__ = \"pandas\"  # specify data type to return from read file\n",
    "# folder or key path\n",
    "__local_folder_path__ = \"rezaware/data/etl/loader/sampledata/\"\n",
    "# __aws_folder_path__ = \"wrangler/data/hospitality/bookings/scraper/\"\n",
    "# __gcs_folder_path__ = \"coingeko\"\n",
    "__local_file_name__ = \"sparkfilewls.csv\"  #file name to read from or write data to\n",
    "# __file_name__ = \"scraper-build-scrape-url-list.csv\" #\"room_descriptions.csv\"\n",
    "# __gcs_file_name__ = \"2022-11-21.json\"  #file name to read from or write data to\n",
    "__file_type__ = \"csv\"   # specify file type to write data to or read from\n",
    "''' options key value pairs must match the pyspark standard'''\n",
    "options = {\n",
    "    \"inferSchema\":True,\n",
    "    \"header\":True,\n",
    "    \"delimiter\":\",\",\n",
    "}\n",
    "\n",
    "_data = clsSpark.read_files_to_dtype(\n",
    "    as_type=__as_type__,\n",
    "    folder_path=__local_folder_path__,\n",
    "    file_name=None,#__local_file_name__,\n",
    "    file_type=__file_type__,\n",
    "    **options,\n",
    ")\n",
    "if __as_type__ == 'pandas':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(_data.shape[0],__as_type__))\n",
    "    print(_data.head(5))\n",
    "elif __as_type__ == 'spark':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(_data.count(),__as_type__))\n",
    "    print(_data.show(5,truncate=False))\n",
    "elif __as_type__ == 'dict':\n",
    "    print(\"got back {0} rows with return dtype {1}\"\n",
    "          .format(len(_data),__as_type__))\n",
    "    print(_data[:3])\n",
    "else:\n",
    "    print('Unrecognised __as_type__' % __as_type__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8d890",
   "metadata": {},
   "source": [
    "## Filter and save data in storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a74efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 60 filtered rows to file savetodummy.csv in rezaware/data/etl/loader/sampledata/\n",
      "saved to <class 'str'> \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "_filter_data = _data.loc[_data['asset_symbol'].isin(['1inch','zks'])]\n",
    "__file_name__=\"savetodummy.csv\"\n",
    "print(\"writing %d filtered rows to file %s in %s\" \n",
    "      % (_filter_data.shape[0],__file_name__,__local_folder_path__))\n",
    "write_data=clsSpark.write_data(\n",
    "    file_name=__file_name__,\n",
    "    folder_path=__local_folder_path__,\n",
    "    data=_filter_data\n",
    ")\n",
    "\n",
    "print(\"saved to %s \" % write_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e86256f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def write_read(bucket_name, blob_name):\n",
    "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your new GCS object\n",
    "    # blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    # Mode can be specified as wb/rb for bytes mode.\n",
    "    # See: https://docs.python.org/3/library/io.html\n",
    "#     with blob.open(\"w\") as f:\n",
    "#         f.write(\"Hello world\")\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        print(f.read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
