{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1a19de",
   "metadata": {},
   "source": [
    "# Write all the default Util Refer data to Tables (Tester)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2801e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bda64",
   "metadata": {},
   "source": [
    "## Instantiate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b55dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional REFERENCE-libraries in UTILS-package of LIB-module imported successfully!\n",
      "sparkFile Class initialization complete\n",
      "reference Class initialization complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('mining/')[0])\n",
    "from rezaware.modules.lib.utils import reference as ref\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    ref = importlib.reload(ref)\n",
    "\n",
    "__desc__ = \"example read and write reference data\"\n",
    "kwargs = {\n",
    "    \"DBTYPE\" : 'PostgreSQL',\n",
    "    \"DBNAME\" : 'property',\n",
    "    \"DBSCHEMA\":'curated',\n",
    "    \"DBUSER\" : 'rezaware',\n",
    "    \"DBPSWD\" : 'rezHERO',\n",
    "    \"STOREMODE\":'local-fs',\n",
    "    \"STOREROOT\":proj_dir.split('mining/')[0],\n",
    "}\n",
    "clsRef = ref.dataWorkLoads(desc=__desc__,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d0d8d",
   "metadata": {},
   "source": [
    "## Import util refer data from file to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a7b987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert 0 and upsert 31 into curated.property\n"
     ]
    }
   ],
   "source": [
    "_folder_path = \"mining/data/property/default/util_ref\"\n",
    "_file_type = \"csv\"   # specify file type to write data to or read from\n",
    "_db_schema='curated'\n",
    "_db_name ='property'\n",
    "\n",
    "# _util_ref_sdf = clsRef.import_to_db(\n",
    "ur_ins_, ur_upd_ = clsRef.import_to_db(\n",
    "    folder_path = _folder_path,\n",
    "    file_type = _file_type,\n",
    "    db_name = _db_name,\n",
    "    db_schema = _db_schema,\n",
    ")\n",
    "\n",
    "print(\"Insert %d and upsert %d into %s.%s\" \n",
    "      % (ur_ins_, ur_upd_, _db_schema, _db_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7f330",
   "metadata": {},
   "source": [
    "## DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e9a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKFILE-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKRDBM-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKRDBM-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKFILE-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "sparkFile Class initialization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 14:27:28 WARN Utils: Your hostname, FarmRaider2 resolves to a loopback address: 127.0.1.1; using 192.168.2.85 instead (on interface enp3s0)\n",
      "23/09/17 14:27:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/09/17 14:27:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/17 14:27:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/17 14:27:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "read and write default property reference data to postgresql class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "proj_dir = os.path.abspath(os.pardir)\n",
    "sys.path.insert(1,proj_dir.split('mining/')[0])\n",
    "from rezaware.modules.etl.loader import sparkFile as file\n",
    "from rezaware.modules.etl.loader import sparkRDBM as db\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    db = importlib.reload(db)\n",
    "    file=importlib.reload(file)\n",
    "\n",
    "__desc__ = \"read and write default property reference data to postgresql\"\n",
    "clsFile = file.dataWorkLoads(\n",
    "    desc=__desc__,\n",
    "    f_store_mode= 'local-fs',\n",
    "    f_store_root= proj_dir.split('mining/')[0]\n",
    ")\n",
    "if clsFile.session:\n",
    "    clsFile._session.stop\n",
    "\n",
    "clsSDB = db.dataWorkLoads(\n",
    "    desc=__desc__,\n",
    "    db_type = 'PostgreSQL',\n",
    "    db_driver=None,\n",
    "    db_hostIP=None,\n",
    "    db_port = None,\n",
    "    db_name = 'property',\n",
    "    db_schema='curated',\n",
    "    db_user = 'rezaware',\n",
    "    db_pswd = 'rezHERO',\n",
    "    spark_partitions=None,\n",
    "    spark_format = 'jdbc',\n",
    "    spark_save_mode=None,\n",
    "    spark_jar_dir = None,\n",
    ")\n",
    "print(\"\\n%s class initialization and load complete!\" % __desc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbb816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----+------------------------+-----------+-----------+-----------+-----------+\n",
      "|ref_pk|entity  |category|code|value                   |description|source_uuid|data_source|data_owner |\n",
      "+------+--------+--------+----+------------------------+-----------+-----------+-----------+-----------+\n",
      "|1     |prop_grp|category|HR  |Beach Hotel and Spa     |null       |3008365    |null       |Delta Check|\n",
      "|2     |prop_grp|category|R   |Beach Resort            |null       |8008485    |null       |Delta Check|\n",
      "|3     |prop_grp|category|BB  |Bed & Breakfast         |null       |5538233    |null       |Delta Check|\n",
      "|4     |prop_grp|category|CC  |Camping and Ferienh√§user|null       |7086070    |null       |Delta Check|\n",
      "|5     |prop_grp|category|GH  |Guesthouse              |null       |5381278    |null       |Delta Check|\n",
      "|6     |prop_grp|category|AP  |Apartments              |null       |8077666    |null       |Delta Check|\n",
      "|7     |prop_grp|category|HO  |Hostel                  |null       |8835928    |null       |Delta Check|\n",
      "|8     |prop_grp|category|H   |Hotel                   |null       |6010713    |null       |Delta Check|\n",
      "|9     |prop_grp|category|HA  |Hotel Apartments        |null       |6132458    |null       |Delta Check|\n",
      "|10    |prop_grp|category|H   |Hotel Health and Spa    |null       |8319097    |null       |Delta Check|\n",
      "+------+--------+--------+----+------------------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "__as_type__ = \"spark\"  # specify data type to return from read file\n",
    "''' set the folder path relative the storeRoot '''\n",
    "_folder_path = \"mining/data/property/default/util_ref\"\n",
    "''' set the single file name '''\n",
    "_file_name = None  #file name to read from or write data to; e.g. myfile.csv\n",
    "''' set the file type to read all files of specified type in folder '''\n",
    "_file_type = \"csv\"   # specify file type to write data to or read from\n",
    "''' options key value pairs must match the pyspark standard'''\n",
    "options = {\n",
    "    \"inferSchema\":True,\n",
    "    \"header\":True,\n",
    "    \"delimiter\":\",\",\n",
    "    \"pathGlobFilter\":'*.csv',\n",
    "    \"recursiveFileLookup\":True,\n",
    "}\n",
    "\n",
    "_util_ref_sdf = clsFile.read_files_to_dtype(\n",
    "    as_type=__as_type__,\n",
    "    folder_path=_folder_path,\n",
    "    file_name=_file_name,\n",
    "    file_type=_file_type,\n",
    "    **options,\n",
    ")\n",
    "\n",
    "# _util_ref_sdf.printSchema()\n",
    "\n",
    "_util_ref_sdf.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8c9720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql property database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataWorkLoads @staticmethod <batch_and_upsert> PSQL connection set with <class 'psycopg2.extensions.cursor'> and connection <connection object at 0x7f5b63bc8e00; dsn: 'user=rezaware password=xxx dbname=property host=127.0.0.1 port=5432', closed: 0>\n",
      "dataWorkLoads @staticmethod <batch_and_upsert> PSQL connection set with <class 'psycopg2.extensions.cursor'> and connection <connection object at 0x7f5b63bc8e00; dsn: 'user=rezaware password=xxx dbname=property host=127.0.0.1 port=5432', closed: 0>\n",
      "dataWorkLoads @staticmethod <batch_and_upsert> PSQL connection set with <class 'psycopg2.extensions.cursor'> and connection <connection object at 0x7f5b63bc8e00; dsn: 'user=rezaware password=xxx dbname=property host=127.0.0.1 port=5432', closed: 0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 31 records\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "_db_schema='curated'\n",
    "_db_name ='property'\n",
    "_tbl_name='util_refer'\n",
    "# _pk = ['ref_pk']\n",
    "_pk = 'ref_pk'\n",
    "_cols_not_for_update = ['created_dt','created_by','created_proc']\n",
    "_options={\n",
    "    \"BATCHSIZE\":1000,   # batch size to partition the dtaframe\n",
    "    \"PARTITIONS\":1,    # number of parallel clusters to run\n",
    "    \"OMITCOLS\":_cols_not_for_update,    # columns to be excluded from update\n",
    "    \n",
    "}\n",
    "\n",
    "# emp_RDD = clsSDB.session.sparkContext.emptyRDD()\n",
    "# _query = f\"SELECT * FROM {_db_schema}.{_db_name} WHERE deactivate_dt is NULL LIMIT 1\"\n",
    "tmp_sdf = clsSDB.read_data_from_table(\n",
    "    db_table=_tbl_name,\n",
    ")\n",
    "\n",
    "tmp_sdf = tmp_sdf.drop(*[x for x in tmp_sdf.columns \n",
    "                         if x not in _util_ref_sdf.columns])\n",
    "\n",
    "_util_ref_sdf = clsSDB.session.createDataFrame(\n",
    "    data=_util_ref_sdf.collect(), schema=tmp_sdf.schema)\n",
    "\n",
    "_ins_sdf = _util_ref_sdf.filter(F.col(_pk).isNull() |\n",
    "                                F.col(_pk).isin('','NaN','None','none'))\\\n",
    "                        .select('*')\n",
    "\n",
    "_ins_count=0\n",
    "if _ins_sdf.count()>0:\n",
    "    _ins_sdf=_ins_sdf.drop(_pk)\n",
    "    _records=clsSDB.insert_sdf_into_table(\n",
    "        save_sdf=_ins_sdf,\n",
    "        db_name =_db_name,\n",
    "        db_table=_tbl_name,\n",
    "    )\n",
    "\n",
    "_upd_sdf = _util_ref_sdf.filter(F.col(_pk).isNotNull() |\n",
    "                                ~F.col(_pk).isin('','NaN','None','none'))\\\n",
    "                        .select('*')\n",
    "_upd_count=0\n",
    "if _upd_sdf.count()>0:\n",
    "    _upd_count=clsSDB.upsert_sdf_to_table(\n",
    "    save_sdf=_upd_sdf,\n",
    "    db_name =_db_name,\n",
    "    db_table=_tbl_name,\n",
    "    unique_keys=[_pk],\n",
    "    **_options,\n",
    ")\n",
    "\n",
    "print(\"Upserted %d records\" % (_upd_count+_ins_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
