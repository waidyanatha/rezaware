#####   WRANGLER DEPLOYMENT SETTINGS   #####
#    
#     should be set to suppport the class libraries and the application
#     to use the deplyment specific paramters for the wrangler work loads
#    
#     See the respective sections: Implementation, Security, Database, Spark
#         Timezon, Currency
#
#    author: <nuwan.waidyanatha@rezgateway.com>
#
#####

[MODULES]
#---set of modules to include or exclude to disable simply comment # the irrelevant ones.
#   give a comma separated of list the packages (or submodules) belonging to the modules
#   e.g., myModule = package1, package2, ..., package n
etl = loader,transform
lib = spark,basemap
ml = cluster,dimreduc,natlang,statistics,timeseries


[DATASTORE]
#---set the default data storage mode; i.e. local or cloud file system.
#   e.g., mode = AWS-S3-BUCKET & root = rezaware-wrangler-source-code (bucket name)
#   e.g., mode = LOCAL-FS & root = rezaware (directory name rezaware app is in)
mode = local-fs
root = rezaware

[APP]
#[APPOWNER]
#--organization name
#  default: RezAWARE (rezgateway)
orgname = rezaware
#--org url
#  default: https://rezgateway.com
orgurl = https://rezgateway.com
#--data hosting root location or S3 Bucket/Object
#  default ../data/
orgemail = rezaware@rezgateway.com
hostip = 127.0.0.1
datadir = ../data/

[HOSTS]
hostip = 127.0.0.1
datadir = ../data/

[SPARK]
#--settings to connect to the database to perform work loads '''
#  install and setup spark: https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/
#  also install findspark by running >>> python3 -m pip install findspark
#  to download Postgres JDBC drivers: https://jdbc.postgresql.org/
homedir = /opt/spark_hadoop_3/
bindir = /opt/spark_hadoop_3/bin
config = spark.jars
#jardir = /opt/spark_hadoop_3/jars/postgresql-42.5.0.jar
jardir = /opt/spark_hadoop_3/jars/postgresql-42.6.0.jar
master = local[1]
partitions = 2
format = jdbc
savemode = Append

[AIRFLOW]
#--set the AIRFLOW_HOME directory $path to save the dags
#  default: airflow_home = ~/airflow
airflowdirhome = ~/airflow
#--set the aiflow username and password
#  default: username=rezaware password=rezaware
airflowappadmin = rezaware
airflowapppswd = rezaware
#--set the airflow email to communicate logs and errors to the admin
#  default: airflow_email = admin.rezaware@rezgateway.com
airflowemail = admin.rezaware@rezgateway.com
#--set the airflow database as postgres and mage sure the change
#  aiflow.cfg parameters
#     sql_alchemy_conn =postgresql+psycopg2://airflow@localhost:5432/airflow
#     executor = LocalExecutor
airflowdbuser = rezawareflow
airflowdbpswd = rezawareflow

[AWSAUTH]
#--AWS security key and credentials
credprofile = rezaware
credfilepath = /home/nuwan/.aws/credentials
region = ap-southeast-1
iampolicy = 
iamuser = 

[GOOGLE]
#--Goole Cloud requires a project to be specified
#  in the credentials
projectid=glass-watch-369111

[DATABASE]
#--database types: mysql, postgresql (default: postgres)
dbtype = postgresql
#--hostIP default 127.0.0.1
dbhostip = 127.0.0.1
#--port default 5432
dbport = 5432
#--database driver
#  postgresql for apache spark: 'org.postgresql.Driver'
dbdriver = org.postgresql.Driver
#--database name
dbname = tip
#--schema name
dbschema = warehouse
#--username and password to connect
#  default db_user=postgres, db_pswd = postgres
dbuser = farmraider
dbpswd = spirittribe

[NOSQLDB]
dbtype = mongodb
dbHostIP = 127.0.0.1
dbport = 27017
##--necessary for the spark connector
dbformat = mongo
##--set a default database
dbname = tip
##--authentiction requires username, password, 
##  authentication database, and mechanisem
dbuser = farmraider
dbpswd = spirittribe
dbauthsource = tip-data-sources
dbauthmechanism = SCRAM-SHA-256
##--setting up a keyfile for authentication
dbtls = false
dbtlsCertKeyFile =
dbtlsCAFile =

[TIMEZONE]
default = utc-8:00

[CURRENCY]
default = USD
currabrv = US$

[LOGGER]
#--directory path to store logs
#  defaulat logs/
path = logs/
file = app.log
#--logging level to set DEBUG, ERROR, WARNING, INFO
level = DEBUG
#--append or write over the logs
#  default: 'a' to append, 'w+' to overwrite 
mode = w+
#--loggin format
#  default: %(asctime)s - %(name)s - %(levelname)s - %(message)s'
# logFormat = %(asctime)s - %(name)s - %(levelname)s - %(message)s
format = asctime,name,levelname,message

